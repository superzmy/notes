首先在性能评价中，流量Byte ps（字节每秒）是个没什么用的指标，不要过多去关注，pps（包每秒）才是真正重要的

因为如下事实：在理想网络状态下，一次发送1字节的速度和100字节并不会有什么差距（可能是2、3倍），但体现在bps上就是100倍。
这和硬盘的读取性能是同理的，每次操作都需要大量计算，和发往硬件的控制信号。

称得上高性能的，通常要以MHz为单位，cpu是3000MHz，也就是几百几千个cpu指令要完成一次操作。

我的多线程写单线程读进程内数据流 3.5GHz e3cpu 4+4线程 win7 极限2500万包每秒（25MHz），正常含各种随机业务的工作800万~1200万包每秒，几百cpu clock要完成一次写包操作

netmap 10G网卡：发包 1488万次（用户测试）~1800万次（官方测试）18MHz

ddkp 双核志强 理想8000万次 90MHz（实际上没有什么用），根据业务其实浮动很大。90clock可以完成一个包

windows nodelaysocket WSASend堵塞0.2ms（也许不准），单线程每秒send外网tcp数千包

windows 正常socket send未测

linux网卡udp多线程100万包1MHz每秒

虚拟机ubuntu16（4线程cpu）单进程测试 (N线程写1线程读）
socketpair 1线程×1byte × 1M = 1312ms 0.76MHz

socketpair 2线程×1byte × 1M = 1655ms 1.21MHz

socketpair 3线程×1byte × 1M = 1972ms 1.52MHz

socketpair 4线程×1byte × 1M = 2534ms 1.58MHz


socketpair 1线程×1024byte × 1M = 1946ms 0.51MHz 0.51GB/s

socketpair 2线程×1024byte × 1M = 2821ms 0.71MHz

socketpair 3线程×1024byte × 1M = 4180ms 0.72MHz

socketpair 4线程×1024byte × 1M = 4960ms 0.80MHz



pipe2 1线程×1byte × 1M = 995ms 1.01MHz

pipe2 2线程×1byte × 1M = 1260ms 1.59MHz

pipe2 3线程×1byte × 1M = 1345ms 2.23MHz

pipe2 4线程×1byte × 1M = 2026ms 1.97MHz



pipe2 1线程×1024byte × 1M = 2535ms 0.39MHz

pipe2 2线程×1024byte × 1M = 2821ms 0.71MHz

pipe2 3线程×1024byte × 1M = 3515ms 0.85MHz

pipe2 4线程×1024byte × 1M = 11350ms 0.35MHz



tcp127.0.0.1 1线程×1byte × 1M = 1379ms 0.73MHz

tcp127.0.0.1 2线程×1byte × 1M = 3727ms 0.54MHz

tcp127.0.0.1 3线程×1byte × 1M = 7718ms 0.39MHz

tcp127.0.0.1 4线程×1byte × 1M = 8027ms 0.5MHz



tcp127.0.0.1 1线程×1000byte × 1M = 1413ms 0.71MHz

tcp127.0.0.1 2线程×1000byte × 1M = 3594ms 0.56MHz

tcp127.0.0.1 3线程×1000byte × 1M = 5972ms 0.50MHz

tcp127.0.0.1 4线程×1000byte × 1M = 8483ms 0.47MHz



